{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65691ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\n",
      "dataset length:  5823\n",
      "transformed dataset\n",
      "dataloader\n",
      "dataloader length:  2912\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.ops import box_iou\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the pretrained model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load the VOC2012 dataset\n",
    "dataset = VOCDetection(root='.', year='2012', image_set='val', download=False)\n",
    "print(\"dataset\")\n",
    "\n",
    "# Define the transforms\n",
    "def transform(image, target):\n",
    "    resize = transforms.Resize((800, 800))\n",
    "    image = resize(image)\n",
    "    image = F.to_tensor(image)\n",
    "    \n",
    "    objects = target['annotation']['object']\n",
    "    if isinstance(objects, dict):\n",
    "        objects = [objects]\n",
    "\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    temp, orig_width, orig_height = image.size()\n",
    "    new_width, new_height = image.shape[2], image.shape[1]\n",
    "\n",
    "    for obj in objects:\n",
    "        xmin = float(obj['bndbox']['xmin']) * new_width / orig_width\n",
    "        ymin = float(obj['bndbox']['ymin']) * new_height / orig_height\n",
    "        xmax = float(obj['bndbox']['xmax']) * new_width / orig_width\n",
    "        ymax = float(obj['bndbox']['ymax']) * new_height / orig_height\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # Convert labels to integers\n",
    "        labels.append(int(obj['name'] == 'person'))\n",
    "\n",
    "    boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.int64)\n",
    "    target = {'boxes': boxes, 'labels': labels}\n",
    "    return image, target\n",
    "\n",
    "# Apply the transforms to the dataset\n",
    "print(\"dataset length: \", len(dataset))\n",
    "\n",
    "class VOC_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, target = self.dataset[idx]\n",
    "        if self.transform:\n",
    "            image, target = self.transform(image, target)\n",
    "        return image, target\n",
    "\n",
    "voc_dataset = VOC_Dataset(dataset, transform=transform)\n",
    "\n",
    "print(\"transformed dataset\")\n",
    "\n",
    "# Create a DataLoader\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    images = torch.stack(images, dim=0)\n",
    "    return images, targets\n",
    "\n",
    "dataloader = DataLoader(voc_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "print(\"dataloader\")\n",
    "\n",
    "# Create lists to store true and predicted boxes and labels\n",
    "true_boxes = []\n",
    "true_labels = []\n",
    "pred_boxes = []\n",
    "pred_scores = []\n",
    "\n",
    "print(\"dataloader length: \", len(dataloader))\n",
    "with torch.no_grad():\n",
    "    for images, targets in dataloader:\n",
    "        outputs = model(images)\n",
    "        for target, output in zip(targets, outputs):\n",
    "            true_boxes.append(target['boxes'])\n",
    "            true_labels.append(target['labels'])\n",
    "            pred_boxes.append(output['boxes'])\n",
    "            pred_scores.append(output['scores'])\n",
    "    \n",
    "\n",
    "# Compute the mAP\n",
    "ap = []\n",
    "print(\"true_boxes length:\", len(true_boxes))\n",
    "for true_box, true_label, pred_box, pred_score in zip(true_boxes, true_labels, pred_boxes, pred_scores):\n",
    "    ious = box_iou(true_box, pred_box)\n",
    "    tp = (ious.max(dim=1)[0] > 0.5) * (true_label == 1)\n",
    "    fp = (ious.max(dim=1)[0] > 0.5) * (true_label == 0)\n",
    "    fn = (ious.max(dim=1)[0] <= 0.5) * (true_label == 1)\n",
    "    precision = tp.sum() / (tp.sum() + fp.sum() + 1e-10)\n",
    "    recall = tp.sum() / (tp.sum() + fn.sum() + 1e-10)\n",
    "    ap.append((2 * precision * recall) / (precision + recall + 1e-10))\n",
    "    print(ap)\n",
    "mAP = torch.stack(ap).mean().item()\n",
    "\n",
    "print(f'mAP: {mAP}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a88a2f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0d9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b918c940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
