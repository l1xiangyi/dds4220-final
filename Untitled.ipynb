{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9af4d1fe",
   "metadata": {},
   "source": [
    "# Faster R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dadd301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.29.2)\n",
      "Requirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (0.9.2)\n",
      "Requirement already satisfied: mapcalc in /opt/conda/lib/python3.10/site-packages (0.2.2)\n",
      "Requirement already satisfied: torchvision==0.14.0 in /opt/conda/lib/python3.10/site-packages (0.14.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torchvision==0.14.0) (4.5.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision==0.14.0) (2.29.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision==0.14.0) (1.23.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.14.0) (9.4.0)\n",
      "Requirement already satisfied: torch==1.13.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.14.0) (1.13.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch==1.13.0->torchvision==0.14.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch==1.13.0->torchvision==0.14.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch==1.13.0->torchvision==0.14.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch==1.13.0->torchvision==0.14.0) (8.5.0.96)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchvision==0.14.0) (65.6.3)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchvision==0.14.0) (0.38.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.3.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.14.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.14.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.14.0) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.14.0) (1.26.15)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers timm mapcalc torchvision==0.14.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ef0691",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0ac089a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./VOCtrainval_11-May-2012.tar\n",
      "Extracting ./VOCtrainval_11-May-2012.tar to .\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision import transforms\n",
    "from torchvision.ops import box_iou\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, image, target):\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.to(device)\n",
    "\n",
    "transform = ToTensor()\n",
    "trainval_data = VOCDetection(root=\".\", image_set=\"trainval\", download=True, transforms=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e92d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(trainval_data))\n",
    "val_size = len(trainval_data) - train_size\n",
    "train_data, val_data = random_split(trainval_data, [train_size, val_size])\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    val_data, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "VOC_CLASSES = [\n",
    "    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n",
    "    \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
    "    \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
    "    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
    "]\n",
    "\n",
    "\n",
    "def preprocess_targets(targets):\n",
    "    new_targets = []\n",
    "    for target in targets:\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for obj in target[\"annotation\"][\"object\"]:\n",
    "            bbox = obj[\"bndbox\"]\n",
    "            box = [int(bbox[\"xmin\"]), int(bbox[\"ymin\"]), int(bbox[\"xmax\"]), int(bbox[\"ymax\"])]\n",
    "            boxes.append(box)\n",
    "            labels.append(VOC_CLASSES.index(obj[\"name\"]))\n",
    "        new_targets.append({\"boxes\": torch.tensor(boxes, dtype=torch.float32), \"labels\": torch.tensor(labels, dtype=torch.int64)})\n",
    "    return new_targets\n",
    "\n",
    "def calculate_image_precision(pred_boxes, true_boxes, thresholds = (0.5, 0.55, 0.6, 0.65, 0.7, 0.75)):\n",
    "    precisions = []\n",
    "    for threshold in thresholds:\n",
    "        fp = tp = 0\n",
    "        for pred_box in pred_boxes:\n",
    "            ious = box_iou(true_boxes, pred_box[0].unsqueeze(0))\n",
    "            if torch.any(ious >= threshold):\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        if (tp+fp) == 0:\n",
    "            precision = 0\n",
    "        else:\n",
    "            precision = tp / (tp + fp)\n",
    "        precisions.append(precision)\n",
    "    return 1 - sum(precisions) / len(precisions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bd03caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP: 0.7411896385279938\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    results = []\n",
    "    image_precisions = []\n",
    "    for images, targets in test_loader:\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = preprocess_targets(targets)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        output = model(images)\n",
    "\n",
    "        preds = []\n",
    "        for box, score in zip(output[0]['boxes'], output[0]['scores']):\n",
    "            preds.append((box, score))\n",
    "        image_precision = calculate_image_precision(preds, targets[0]['boxes'])\n",
    "        image_precisions.append(image_precision)\n",
    "\n",
    "    mAP = sum(image_precisions) / len(image_precisions)\n",
    "    print(f'mAP: {mAP}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22af368",
   "metadata": {},
   "source": [
    "## Fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0648010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP: 0.7253286327644746\n"
     ]
    }
   ],
   "source": [
    "# torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "num_classes = 21\n",
    "# Get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, targets in train_loader:\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = preprocess_targets(targets)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Forward pass\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    try:\n",
    "        return tuple(zip(*batch))\n",
    "    except Exception as e:\n",
    "        print(f\"Exception while processing batch: {e}\")\n",
    "        for item in batch:\n",
    "            try:\n",
    "                # Attempt to process each item in the batch individually\n",
    "                tuple(zip(*item))\n",
    "            except Exception as e:\n",
    "                print(f\"Exception for item {item}: {e}\")\n",
    "        # Reraise the original exception\n",
    "        raise e\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_data, batch_size=2, shuffle=True, num_workers=4,\n",
    "#     collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, optimizer, num_epochs=2)\n",
    "\n",
    "# After training, save the model for future use or evaluation\n",
    "torch.save(model.state_dict(), 'fasterrcnn_resnet50_fpn_finetuned.pth')\n",
    "\n",
    "# Now evaluate the model with the evaluation code\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    results = []\n",
    "    image_precisions = []\n",
    "    for images, targets in test_loader:\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = preprocess_targets(targets)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        output = model(images)\n",
    "\n",
    "        preds = []\n",
    "        for box, score in zip(output[0]['boxes'], output[0]['scores']):\n",
    "            preds.append((box, score))\n",
    "        image_precision = calculate_image_precision(preds, targets[0]['boxes'])\n",
    "        image_precisions.append(image_precision)\n",
    "\n",
    "    # Compute mean Average Precision (mAP)\n",
    "    mAP = sum(image_precisions) / len(image_precisions)\n",
    "    print(f'mAP: {mAP}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca4ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973a074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab58172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
