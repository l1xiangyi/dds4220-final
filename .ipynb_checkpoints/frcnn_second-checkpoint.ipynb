{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65691ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.ops import box_iou\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the pretrained model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Load the VOC2012 dataset\n",
    "dataset = VOCDetection(root='.', year='2012', image_set='val', download=False)\n",
    "\n",
    "# Define the transforms\n",
    "def transform(data):\n",
    "    image, target = data\n",
    "    resize = transforms.Resize((800, 800))\n",
    "    image = resize(image)\n",
    "    image = F.to_tensor(image)\n",
    "    \n",
    "    objects = target['annotation']['object']\n",
    "    if isinstance(objects, dict):\n",
    "        objects = [objects]\n",
    "\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    temp, orig_width, orig_height = image.size()\n",
    "    new_width, new_height = image.shape[2], image.shape[1]\n",
    "\n",
    "    for obj in objects:\n",
    "        xmin = float(obj['bndbox']['xmin']) * new_width / orig_width\n",
    "        ymin = float(obj['bndbox']['ymin']) * new_height / orig_height\n",
    "        xmax = float(obj['bndbox']['xmax']) * new_width / orig_width\n",
    "        ymax = float(obj['bndbox']['ymax']) * new_height / orig_height\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # Convert labels to integers\n",
    "        labels.append(int(obj['name'] == 'person'))\n",
    "\n",
    "    boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.int64)\n",
    "    target = {'boxes': boxes, 'labels': labels}\n",
    "    return image, target\n",
    "\n",
    "# Apply the transforms to the dataset\n",
    "dataset = [(transform(data)) for data in dataset]\n",
    "\n",
    "# Create a DataLoader\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    images = torch.stack(images, dim=0)\n",
    "    return images, targets\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Create lists to store true and predicted boxes and labels\n",
    "true_boxes = []\n",
    "true_labels = []\n",
    "pred_boxes = []\n",
    "pred_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in dataloader:\n",
    "        outputs = model(images)\n",
    "        for target, output in zip(targets, outputs):\n",
    "            true_boxes.append(target['boxes'])\n",
    "            true_labels.append(target['labels'])\n",
    "            pred_boxes.append(output['boxes'])\n",
    "            pred_scores.append(output['scores'])\n",
    "\n",
    "# Compute the mAP\n",
    "ap = []\n",
    "for true_box, true_label, pred_box, pred_score in zip(true_boxes, true_labels, pred_boxes, pred_scores):\n",
    "    ious = box_iou(true_box, pred_box)\n",
    "    tp = (ious.max(dim=1)[0] > 0.5) * (true_label == 1)\n",
    "    fp = (ious.max(dim=1)[0] > 0.5) * (true_label == 0)\n",
    "    fn = (ious.max(dim=1)[0] <= 0.5) * (true_label == 1)\n",
    "    precision = tp.sum() / (tp.sum() + fp.sum() + 1e-10)\n",
    "    recall = tp.sum() / (tp.sum() + fn.sum() + 1e-10)\n",
    "    ap.append((2 * precision * recall) / (precision + recall + 1e-10))\n",
    "mAP = torch.stack(ap).mean().item()\n",
    "\n",
    "print(f'mAP: {mAP}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a88a2f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0d9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b918c940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
